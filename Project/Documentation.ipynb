{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "136e7adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jrh25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jrh25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d097aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and', 'isn', 'off', \"doesn't\", 'then', 'shouldn', 'too', 'why', 'o', 'out', 'won', 'each', 'they', 'both', 'whom', 'at', 'there', 'who', 'y', 'doing', 'where', 'our', 'nor', 'these', 'this', 'few', 'after', 'has', 'not', 'couldn', 'down', 'theirs', 'or', 'now', 'himself', 'the', 'myself', 'hasn', 'll', \"mustn't\", \"should've\", 'for', 'themselves', \"you'd\", 'me', 'm', 'being', \"you've\", 'under', 'some', 'her', 'against', 'above', \"mightn't\", 'other', 'ain', 'having', 'it', \"couldn't\", 'its', 'on', 'again', \"isn't\", 'before', 'below', 'if', 'which', 'until', 'into', 'itself', 'only', 'yourselves', 'should', \"don't\", 'you', 'any', 'here', \"wouldn't\", 'most', 'yours', 'because', 'up', 'by', 'just', \"hasn't\", 'further', 's', 'were', 'their', 'so', 'weren', \"shan't\", 'about', 'herself', 'while', 'is', \"won't\", \"she's\", 'am', 'd', 'his', 'will', 'those', 'him', \"aren't\", \"needn't\", \"weren't\", 'from', 'haven', 'your', 'through', 're', 'with', 'between', 'yourself', 'didn', 'of', 'was', 'did', 'we', 'wouldn', 'them', 'hadn', \"it's\", \"you're\", 'aren', 'be', 'i', 'wasn', 'when', 'been', 'do', 'over', 'ours', 'ma', \"you'll\", 'what', 'more', 'once', \"wasn't\", 'all', 'my', 'needn', 'mustn', 'he', 'a', 'as', 'during', 'shan', 'own', 'does', 'can', 'no', 'to', 'ourselves', 'very', 't', 'such', 'are', 'than', \"haven't\", 'same', 've', \"shouldn't\", 'an', 'had', 'doesn', 'hers', 'in', \"hadn't\", 'mightn', \"didn't\", 'but', \"that'll\", 'have', 'don', 'she', 'that', 'how'}\n"
     ]
    }
   ],
   "source": [
    "# stopwords\n",
    "# stop words are basically all the extraneous words in each sentence that we can feasibly ignore\n",
    "# this would (ideally) leave us with just the main nouns, verbs and such in the sentence\n",
    "# we will use this in later functions\n",
    "# note that in the end, stopwords is literally a list of useless words\n",
    "# that means that we can make our own list of words, characters, or punctuation to ignore in the future\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0c1821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean\n",
      "clean\n"
     ]
    }
   ],
   "source": [
    "# PorterStemmer()\n",
    "# PorterStemmer is a class that gives us access to useful functions. \n",
    "# For the purpose of this algorithm, we use stem(), which simply diminishes a word to its root base. \n",
    "# For example, \"cleaning\" and \"cleaned\" would both return as \"clean\"\n",
    "\n",
    "# We first have to declare an instance of the PorterStemmer() class for use.\n",
    "stem = PorterStemmer()\n",
    "wd = stem.stem(\"cleaning\")\n",
    "print(wd)\n",
    "wd1 = stem.stem(\"cleaned\")\n",
    "print(wd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e1e97ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sally sells seashells by the seashore.\n",
      "She worries that she doesn't have enough seashells to sell on the seashore.\n",
      "She won't give up.\n",
      "Sally wants to sell seashells on the seashore, because she likes to sell seashells on the seashore.\n",
      "She is selling seashells on the seashell shore.\n",
      "How much wood could the woodchuck chuck if the woodchuck could chuck wood?\n",
      "She sells seashells by the seashore.\n"
     ]
    }
   ],
   "source": [
    "# sent_tokenize\n",
    "# processes a paragraph into a list with multiple sentences\n",
    "# for the purposes of the algorithm, we use this instead of split\n",
    "paragraphContent = \"\"\"Sally sells seashells by the seashore. She worries that she doesn't have enough seashells to sell on the seashore. She won't give up. Sally wants to sell seashells on the seashore, because she likes to sell seashells on the seashore. She is selling seashells on the seashell shore. How much wood could the woodchuck chuck if the woodchuck could chuck wood? She sells seashells by the seashore.\"\"\"\n",
    "allSentences = sent_tokenize(paragraphContent)\n",
    "for i in allSentences:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ac170a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "an\n",
      "example\n",
      "sentence\n",
      ",\n",
      "it\n",
      "is\n",
      "n't\n",
      "anything\n",
      "complicated\n",
      ".\n",
      "\n",
      "\n",
      "['Sally', 'sells', 'seashells', 'by', 'the', 'seashore', '.']\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize\n",
    "# processes a sentence into a list with all words and punctuation in the sentence\n",
    "# for the purposes of the algorithm we use this instead of split\n",
    "sentence = \"This is an example sentence, it isn't anything complicated.\"\n",
    "words = word_tokenize(sentence)\n",
    "for i in words:\n",
    "    print(i)\n",
    "\n",
    "\n",
    "sentence = \"Sally sells seashells by the seashore.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(\"\\n\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c7057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
