{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 208,
=======
   "execution_count": 1,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "id": "1a015e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the resources doc if you haven't already!\n",
    "# Also please add whatever interesting/useful sources you find on there for everyone else!\n",
    "# Also, PLEASE check the documentation file in the Project folder if you are confused about any of the NLTK functions\n",
    "\n",
    "# Text summarization tends to have two approaches: extraction and abstraction\n",
    "# Because abstraction is more complex, we can try to build an extraction algorithm first\n",
    "# Although I would definitely recommend checking it out to get an idea of how we can go forward from here"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 209,
=======
   "execution_count": 2,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "id": "6c4c7441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For collaborators, right now I'm using the algorithm here:\n",
    "# https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 210,
=======
   "execution_count": 3,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "id": "c48df224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TLDR; Our extraction algorithm will go like this:\n",
    "# -obtain data\n",
    "# -process text\n",
    "# -tokenization\n",
    "# -find weighted frequency of words (weigh by sentence length, paragraph length, etc)\n",
    "# -substitute words with their weighted frequencies (exactly what it sounds like)\n",
    "# -sum up the weighted frequencies in each sentence, and the sentences with highest sums make up our summary"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 211,
=======
   "execution_count": 4,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "id": "b967d231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jrh25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jrh25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Starting off with imports\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 212,
=======
   "execution_count": 5,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "id": "a2410f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should try to make this less memory intensive\n",
    "# paragraphContent should be fetched from database file\n",
    "paragraphContent = \"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 213,
=======
   "execution_count": 6,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "id": "3e131349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create word frequency table for the entire text of the paragraph\n",
    "def create_frequency_table(content):\n",
    "    frequency_table = {}\n",
    "    word_list = word_tokenize(content)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stem = PorterStemmer()\n",
    "    for word in word_list:\n",
    "        word = stem.stem(word)\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        if word in frequency_table: \n",
    "            frequency_table[word] += 1\n",
    "        else: \n",
    "            frequency_table[word] = 1\n",
    "# making punctuation have 0 frequency to prevent them from skewing our weighted frequency\n",
    "    punctuation = {\";\", \":\", \"'\", \".\", \",\", \"!\", \"?\", \"(\", \")\"}\n",
    "    for word in frequency_table:\n",
    "        if word in punctuation:\n",
    "            frequency_table[word] = 0\n",
    "    return frequency_table"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 214,
=======
   "execution_count": 7,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "id": "83feb869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in the word frequency dictionary from create_dictionary_table\n",
    "def create_weighted_table(frequency_table):\n",
    "    weighted_frequency_table = {}\n",
    "    highestfreq = max(frequency_table.values())\n",
    "    for word in frequency_table:\n",
    "        weighted_frequency_table[word] = frequency_table[word] / highestfreq\n",
    "    return weighted_frequency_table"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 215,
=======
   "execution_count": 8,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "id": "7bee0cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutParagraph(content):\n",
    "    allSentences = sent_tokenize(content)\n",
    "    return allSentences;"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 216,
=======
   "execution_count": 9,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "id": "0e69fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitute words in each sentence with weighted frequencies, \n",
    "# sum up the weighted word frequencies in each sentence, these sums will be the sentence's \"score\"\n",
    "# compare the sentence scores, and grab the ones with the highest scores for our summary"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 217,
=======
   "execution_count": 10,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "id": "924e2ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean sentences using PorterStemmer() and getting rid of stop words, similar to tokenization\n",
    "# we can then calculate the scores in each sentence based on the \"weighted frequencies\" that we\n",
    "# assigned to each of these words in our previously made dictionary\n",
    "def clean_sentences(content):\n",
    "    allSentences = sent_tokenize(content)\n",
    "    stem = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    all_clean_sentences = []\n",
    "    for sentence in allSentences:\n",
    "        cleaned_sentence = []\n",
    "        word_list = word_tokenize(sentence)\n",
    "        for word in word_list:\n",
    "            word = stem.stem(word)\n",
    "            if (word not in stop_words):\n",
    "                cleaned_sentence.append(word)\n",
    "        all_clean_sentences.append(cleaned_sentence)\n",
    "    return all_clean_sentences"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 218,
=======
   "execution_count": 11,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "id": "7e2e83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the frequencies of each sentence as a sentence score, and returns the sentence with the greatest frequency \n",
    "\n",
    "def one_sentence_summary(content):\n",
    "    freq_dict = create_frequency_table(content)\n",
    "    weighted_dict = create_weighted_table(freq_dict)\n",
    "    cleanedSentences = clean_sentences(content)\n",
    "    addedNum = []\n",
    "    \n",
    "    for clean_sentence in cleanedSentences:\n",
    "        num = 0\n",
    "        for token in clean_sentence:\n",
    "            if token in weighted_dict.keys():\n",
    "                num = num + weighted_dict[token]\n",
    "        addedNum.append(num)\n",
    "        \n",
    "    largest = addedNum[0]\n",
    "    numTracker = 0\n",
    "    finalNum = 0\n",
    "    for i in addedNum:\n",
    "        numTracker = numTracker + 1\n",
    "        if i > largest:\n",
    "            largest = i\n",
    "            finalNum = numTracker\n",
    "    # finalNum is the index of the sentence that has the highest score\n",
    "    allSentences = cutParagraph(content)\n",
    "    return allSentences[finalNum - 1]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 219,
=======
   "execution_count": 12,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "id": "d9935c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "content1 = \"\"\"Sally sells seashells by the seashore. She worries that she doesn't have enough seashells to sell on the seashore. She won't give up. Sally wants to sell seashells on the seashore, because she likes to sell seashells on the seashore. She is selling seashells on the seashell shore. How much wood could the woodchuck chuck if the woodchuck could chuck wood? She sells seashells by the seashore.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 220,
=======
   "execution_count": 13,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "id": "3880c04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sally wants to sell seashells on the seashore, because she likes to sell seashells on the seashore.'"
      ]
     },
<<<<<<< HEAD
     "execution_count": 220,
=======
     "execution_count": 13,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_sentence_summary(content1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 221,
=======
   "execution_count": 14,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "id": "aa3bd95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(content, degree_of_summarization):\n",
    "    # this time, we are able to summarize larger bodies of text\n",
    "    # we also give the user control over how general the degree of summarization is with another parameter\n",
    "    # degree_of_summarization should be an integer between 0 and 10, with 10 being the highest degree of summarization\n",
    "    # 10 would output a one sentence summary\n",
    "    freq_dict = create_frequency_table(content)\n",
    "    weighted_dict = create_weighted_table(freq_dict)\n",
    "    cleanedSentences = clean_sentences(content)\n",
    "    addedNum = []\n",
    "    \n",
    "    for clean_sentence in cleanedSentences:\n",
    "        num = 0\n",
    "        for token in clean_sentence:\n",
    "            if token in weighted_dict.keys():\n",
    "                num = num + weighted_dict[token]\n",
    "        addedNum.append(num)\n",
    "\n",
    "    # find largest sentence score to compare others against\n",
    "    largest_score = addedNum[0]\n",
    "    for num in addedNum:\n",
    "        if num > largest_score:\n",
    "            largest_score = num\n",
    "\n",
    "    # start collecting sentences\n",
    "    index_list = []\n",
    "    deg = degree_of_summarization\n",
    "    for i in range(len(addedNum)):\n",
    "        if addedNum[i] >= (largest_score * (deg / 10)):\n",
    "            index_list.append(i)\n",
    "    allSentences = cutParagraph(content)\n",
    "    \n",
    "    summary = \"\"\"\"\"\"\n",
    "    for i in index_list:\n",
    "        summary = summary + \" \" + allSentences[i]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 222,
=======
   "execution_count": 15,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "id": "a93830f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyWords(content):\n",
    "    freq_dict = create_frequency_table(content)\n",
    "    weighted_dict = create_weighted_table(freq_dict)\n",
    "    Sentences = cutParagraph(content)\n",
    "    words = word_tokenize(content)\n",
    "    ps = PorterStemmer()\n",
<<<<<<< HEAD
=======
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    #for word in words:\n",
    "    #    if word in stop_words:\n",
    "    #        words.remove(word)\n",
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
    "    \n",
    "    finalThree = []\n",
    "    finalThreeSt = []\n",
    "    mass = 0\n",
    "    value = \"\"\n",
    "    value2 = \"\"\n",
<<<<<<< HEAD
=======
    "    \n",
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
    "    for j in words:\n",
    "        for i in weighted_dict.keys():\n",
    "            if ps.stem(j) == i:\n",
    "                if weighted_dict[i] > mass:\n",
    "                    mass = weighted_dict[i]\n",
    "                    value = j\n",
    "                    value2 = i\n",
    "    finalThree.append(value)\n",
    "    finalThreeSt.append(value2)\n",
    "    \n",
    "    mass = 0\n",
    "    for j in words:\n",
    "        for i in weighted_dict.keys():\n",
    "            if ps.stem(j) == i:\n",
    "                if weighted_dict[i] > mass and i != finalThreeSt[0]:\n",
    "                    mass = weighted_dict[i]\n",
    "                    value = j\n",
    "                    value2 = i\n",
    "    finalThree.append(value)\n",
    "    finalThreeSt.append(value2)\n",
    "    \n",
    "    mass = 0\n",
<<<<<<< HEAD
    "    for j in words:\n",
    "        for i in weighted_dict.keys():\n",
    "            if ps.stem(j) == i:\n",
    "                if weighted_dict[i] > mass and i != finalThreeSt[0] and i != finalThreeSt[1]:\n",
    "                    mass = weighted_dict[i]\n",
    "                    value = j\n",
    "                    value2 = i\n",
=======
    "    \n",
    "    for i in weighted_dict.keys():\n",
    "        if weighted_dict[i] > mass and i != finalThree[0] and i != finalThree[1]:\n",
    "            mass = weighted_dict[i]\n",
    "            value = i\n",
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
    "    finalThree.append(value)\n",
    "    finalThreeSt.append(value2)\n",
    "    \n",
    "    return finalThree\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 223,
=======
   "execution_count": 16,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "id": "e3fa92b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "['seashells', 'sells', 'seashore']"
      ]
     },
     "execution_count": 223,
=======
       "['seashells', 'sells', 'seashel']"
      ]
     },
     "execution_count": 16,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyWords(content1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 224,
   "id": "03a06539",
=======
   "execution_count": 17,
   "id": "7538b1da",
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "metadata": {},
   "outputs": [],
   "source": [
    "content3 = \"\"\"Greg understood that this situation would make Michael terribly uncomfortable. Michael simply had no idea what was about to come and even though Greg could prevent it from happening, he opted to let it happen. It was quite ironic, really. It was something Greg had said he would never wish upon anyone a million times, yet here he was knowingly letting it happen to one of his best friends. He rationalized that it would ultimately make Michael a better person and that no matter how uncomfortable, everyone should experience racism at least once in their lifetime.\n",
    "They say you only come to peace with yourself when you know yourself better than those around you. Derick knew nothing about this. He thought he had found peace but this was an illusion as he was about to find out with an unexpected occurrence that he actually knew nothing about himself.\n",
    "It really shouldn't have mattered to Betty. That's what she kept trying to convince herself even if she knew it mattered to Betty more than practically anything else. Why was she trying to convince herself otherwise? As she stepped forward to knock on Betty's door, she still didn't have a convincing answer to this question that she'd been asking herself for more than two years now.\n",
    "At that moment he had a thought that he'd never imagine he'd consider. \"I could just cheat,\" he thought, \"and that would solve the problem.\" He tried to move on from the thought but it was persistent. It didn't want to go away and, if he was honest with himself, he didn't want it to.\n",
    "Puppies are soft, cute, funny, and make a big mess. Every month or two our family fosters 6-12 week old puppies for a puppy rescue nonprofit organization. We all enjoy cuddling their furry bodies after a clean bath. Fresh puppy smell is great. The puppies play with each other and our adult dog. They look so funny when they lay on top of each other and sleep. While puppies can be great fun, they also can make big messes. 4-6 puppies can make a lot of puppy pee and poop. It's a challenge to keep the puppies and the puppy pen clean.\n",
    "Patrick didn't want to go. The fact that she was insisting they must go made him want to go even less. He had no desire to make small talk with strangers he would never again see just to be polite. But she insisted that Patrick go, and she would soon find out that this would be the biggest mistake she could make in their relationship.\n",
    "The day had begun on a bright note. The sun finally peeked through the rain for the first time in a week, and the birds were singing in its warmth. There was no way to anticipate what was about to happen. It was a worst-case scenario and there was no way out of it.\n",
    "The words hadn't flowed from his fingers for the past few weeks. He never imagined he'd find himself with writer's block, but here he sat with a blank screen in front of him. That blank screen taunting him day after day had started to play with his mind. He didn't understand why he couldn't even type a single word, just one to begin the process and build from there. And yet, he already knew that the eight hours he was prepared to sit in front of his computer today would end with the screen remaining blank.\"\"\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 225,
   "id": "eeac02d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Five seconds, four, three, two, one... Don't forget that gifts often come with costs that go beyond their purchase price.\""
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(content2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "3ec95d07",
=======
   "execution_count": 18,
   "id": "a3064ed7",
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Michael simply had no idea what was about to come and even though Greg could prevent it from happening, he opted to let it happen. It was something Greg had said he would never wish upon anyone a million times, yet here he was knowingly letting it happen to one of his best friends. He thought he had found peace but this was an illusion as he was about to find out with an unexpected occurrence that he actually knew nothing about himself. As she stepped forward to knock on Betty's door, she still didn't have a convincing answer to this question that she'd been asking herself for more than two years now. It didn't want to go away and, if he was honest with himself, he didn't want it to. The fact that she was insisting they must go made him want to go even less. But she insisted that Patrick go, and she would soon find out that this would be the biggest mistake she could make in their relationship. There was no way to anticipate what was about to happen. He didn't understand why he couldn't even type a single word, just one to begin the process and build from there. And yet, he already knew that the eight hours he was prepared to sit in front of his computer today would end with the screen remaining blank.\""
      ]
     },
<<<<<<< HEAD
     "execution_count": 226,
=======
     "execution_count": 18,
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(content3, 5)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 227,
   "id": "32bd9529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Twenty seconds were all that was left and Richard could hear each one tick by. Ten seconds and he wasn't sure he had enough time. Five seconds, four, three, two, one... Don't forget that gifts often come with costs that go beyond their purchase price. When you purchase the latest gaming system, you're likely not going to be satisfied with the games that come with it for long and want to purchase new titles to play. When you buy gifts it's important to remember that some come with additional costs down the road that can be much more expensive than the initial gift itself. He wasn't sure whether he liked it or not, but at this moment it didn't matter. She made him a new-fangled drink each day and he took a sip of it and smiled, saying it was excellent.The bush began to shake. Brad couldn't see what was causing it to shake, but he didn't care. He had a pretty good idea about what was going on and what was happening. He was so confident that he approached the bush carefree and with a smile on his face. But she didn't because she knew his friends were all that he had and even if that meant a life of misery, he would never give them up.\""
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(content2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c0fa444f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Twenty seconds were all that was left and Richard could hear each one tick by. Fifteen seconds now remained and the panic began to fully set in. Ten seconds and he wasn\\'t sure he had enough time. Five seconds, four, three, two, one... Don\\'t forget that gifts often come with costs that go beyond their purchase price. When you purchase a child the latest smartphone, you\\'re also committing to a monthly phone bill. When you purchase the latest gaming system, you\\'re likely not going to be satisfied with the games that come with it for long and want to purchase new titles to play. When you buy gifts it\\'s important to remember that some come with additional costs down the road that can be much more expensive than the initial gift itself. He wasn\\'t sure whether he liked it or not, but at this moment it didn\\'t matter. She had made it especially for him so he would have forced it down even if he had absolutely hated it. She made him a new-fangled drink each day and he took a sip of it and smiled, saying it was excellent.The bush began to shake. Brad couldn\\'t see what was causing it to shake, but he didn\\'t care. He had a pretty good idea about what was going on and what was happening. He was so confident that he approached the bush carefree and with a smile on his face. That all changed the instant he realized what was actually behind the bush. She looked at her student wondering if she could ever get through. \"You need to learn to think for yourself,\" she wanted to tell him. \"Your friends are holding you back and bringing you down.\" But she didn\\'t because she knew his friends were all that he had and even if that meant a life of misery, he would never give them up.'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For some reason, any degree of summarization that's less than 3 returns a summary where every contraction has a backslash in them\n",
    "# I still haven't found the reason for that\n",
    "# For demo purposes, let's just keep the degree of summarization above 4 or so.\n",
    "\n",
    "summarize(content2, 2)"
=======
   "execution_count": 23,
   "id": "c26fe0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",seashells,sells,seashel\n",
      "['', 'seashells', 'sells', 'seashel']\n"
     ]
    }
   ],
   "source": [
    "temp = \"\"\n",
    "temp1 = keyWords(content1)\n",
    "for word in temp1:\n",
    "    temp = temp + \",\" + word\n",
    "keywords = temp\n",
    "print(keywords)\n",
    "print(keywords.split(\",\"))"
>>>>>>> 9cdbe0581af1107fa740803a9763be281cedbf37
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab67473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
