{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a015e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the resources doc if you haven't already!\n",
    "# Also please add whatever interesting/useful sources you find on there for everyone else!\n",
    "\n",
    "# Text summarization tends to have two approaches: extraction and abstraction\n",
    "# Because abstraction is more complex, we can try to build an extraction algorithm first\n",
    "# Although I would definitely recommend checking it out to get an idea of how we can go forward from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c4c7441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For collaborators, right now I'm using the algorithm here:\n",
    "# https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48df224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TLDR; Our extraction algorithm will go like this:\n",
    "# -obtain data\n",
    "# -process text\n",
    "# -tokenization\n",
    "# -find weighted frequency of words (weigh by sentence length, paragraph length, etc)\n",
    "# -substitute words with their weighted frequencies (exactly what it sounds like)\n",
    "# -sum up the weighted frequencies in each sentence, and the sentences with highest sums make up our summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b967d231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jrh25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jrh25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Starting off with import\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6baa01d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sally', 'sells', 'seashells', 'by', 'the', 'seashore', '.']\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize breaks the sentence into individual components\n",
    "text = \"Sally sells seashells by the seashore.\"\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2410f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should try to make this less memory intensive\n",
    "# paragraphContent should be fetched from database file, I fill it in for testing purposes\n",
    "paragraphContent = \"\"\"Sally sells seashells by the seashore. She sells seashells on the seashell shore. How much wood could the woodchuck chuck if the woodchuck could chuck wood? She sells seashells by the seashore.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfd2498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutParagraph(content):\n",
    "    allSentences = sent_tokenize(content)\n",
    "    return allSentences;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "885a3410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sally sells seashells by the seashore.\n",
      "She sells seashells on the seashell shore.\n",
      "How much wood could the woodchuck chuck if the woodchuck could chuck wood?\n",
      "She sells seashells by the seashore.\n"
     ]
    }
   ],
   "source": [
    "# sent_tokenize (sentence tokenize) breaks a paragraph into its individual sentences\n",
    "cut = cutParagraph(paragraphContent)\n",
    "for i in cut: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4851e82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'out', 'they', 'before', 'why', 'will', 'these', 'their', 'if', 'off', 'our', 'because', 'for', 'but', \"isn't\", 'whom', 'where', 'just', 'when', 'or', 'shouldn', \"shouldn't\", 'during', 'up', 'than', 'her', 'had', 'himself', \"hadn't\", 'same', 'those', \"shan't\", 'haven', 'shan', 'do', 'there', 'me', 'above', 'didn', 'other', \"it's\", 'ourselves', 'further', 'under', 't', 'she', 'not', 'be', 's', 'aren', 'am', 've', 'i', 'theirs', 'what', \"aren't\", 'you', 'here', 'down', 'to', 'below', 'your', 'ma', 'of', 'being', 'any', \"wasn't\", 'does', 'how', 'very', 'which', 'having', \"couldn't\", 'nor', 'again', 'so', 'ours', 'herself', 'some', 'been', 'an', 'can', 'it', 'at', \"needn't\", 'its', 'mightn', 'each', \"hasn't\", \"haven't\", 'themselves', 'over', 'wasn', 'until', 'by', 'once', 'yourself', \"you'd\", 'about', 'is', 'll', 'all', 'should', \"didn't\", 'such', 'few', \"you'll\", \"should've\", 'no', 'won', 'from', 'as', 'have', 'we', 'yourselves', 'm', 'now', 'into', 'weren', 'needn', 'itself', 'doing', 'too', 'his', 'wouldn', 'he', \"won't\", 'then', 'doesn', 'him', \"that'll\", 'that', 'my', 'are', 'don', \"doesn't\", 'hadn', 'was', 'while', \"she's\", 'through', \"you're\", 'who', 'between', 'were', 'couldn', 'yours', 'in', 'y', 'with', 'the', 're', 'isn', \"you've\", 'only', 'them', 'on', 'against', 'own', \"weren't\", 'did', 'and', \"wouldn't\", 'this', \"mightn't\", 'more', \"don't\", 'hers', \"mustn't\", 'both', 'mustn', 'after', 'myself', 'has', 'a', 'd', 'most', 'o', 'hasn', 'ain'}\n"
     ]
    }
   ],
   "source": [
    "# stop words are basically all the extraneous words in each sentence that we can feasibly ignore\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e131349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a108c0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
