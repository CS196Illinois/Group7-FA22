{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a015e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the resources doc if you haven't already!\n",
    "# Also please add whatever interesting/useful sources you find on there for everyone else!\n",
    "# Also, PLEASE check the documentation file in the Project folder if you are confused about any of the NLTK functions\n",
    "\n",
    "# Text summarization tends to have two approaches: extraction and abstraction\n",
    "# Because abstraction is more complex, we can try to build an extraction algorithm first\n",
    "# Although I would definitely recommend checking it out to get an idea of how we can go forward from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c4c7441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For collaborators, right now I'm using the algorithm here:\n",
    "# https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48df224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TLDR; Our extraction algorithm will go like this:\n",
    "# -obtain data\n",
    "# -process text\n",
    "# -tokenization\n",
    "# -find weighted frequency of words (weigh by sentence length, paragraph length, etc)\n",
    "# -substitute words with their weighted frequencies (exactly what it sounds like)\n",
    "# -sum up the weighted frequencies in each sentence, and the sentences with highest sums make up our summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b967d231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jrh25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jrh25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Starting off with import\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6baa01d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sally', 'sells', 'seashells', 'by', 'the', 'seashore', '.']\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize breaks the sentence into individual components\n",
    "text = \"Sally sells seashells by the seashore.\"\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2410f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should try to make this less memory intensive\n",
    "# paragraphContent should be fetched from database file, I fill it in for testing purposes\n",
    "paragraphContent = \"\"\"Sally sells seashells by the seashore. She worries that she doesn't have enough seashells to sell on the seashore. She won't give up. Sally wants to sell seashells on the seashore, because she likes to sell seashells on the seashore. She is selling seashells on the seashell shore. How much wood could the woodchuck chuck if the woodchuck could chuck wood? She sells seashells by the seashore.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfd2498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutParagraph(content):\n",
    "    allSentences = sent_tokenize(content)\n",
    "    return allSentences;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "885a3410",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sally sells seashells by the seashore.\n",
      "She worries that she doesn't have enough seashells to sell on the seashore.\n",
      "She won't give up.\n",
      "Sally wants to sell seashells on the seashore, because she likes to sell seashells on the seashore.\n",
      "She is selling seashells on the seashell shore.\n",
      "How much wood could the woodchuck chuck if the woodchuck could chuck wood?\n",
      "She sells seashells by the seashore.\n"
     ]
    }
   ],
   "source": [
    "# sent_tokenize (sentence tokenize) breaks a paragraph into its individual sentences\n",
    "allSentences = cutParagraph(paragraphContent)\n",
    "for i in allSentences: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4851e82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ours', 'she', 'doesn', \"should've\", 'needn', 'on', 'was', 'hasn', 'weren', 'about', 'up', 'me', 'now', \"aren't\", \"mightn't\", 'been', 'does', 'this', 'against', 'he', 'i', 'theirs', 'being', \"hasn't\", 'more', 'few', 'it', 'each', 'themselves', 'our', 'from', 'same', 'have', 'how', 'itself', 'nor', 'because', 'that', 'than', 'do', 'when', 'very', \"mustn't\", 'ma', 'can', 'them', \"weren't\", 'o', 'any', \"doesn't\", 'are', 'for', 'were', 'no', 'over', \"isn't\", 'again', 'don', 'there', 'they', 're', 'has', 'with', 'where', 'their', 'so', \"couldn't\", 'between', 'other', 'himself', 'my', 'under', 'an', 'off', 'before', 'mightn', 'm', 'your', \"you've\", 'while', 'whom', \"don't\", 'should', 'of', 'who', 'didn', 'yours', 'hadn', \"needn't\", 's', 'having', 'him', \"you'd\", 'once', \"wouldn't\", \"she's\", 'as', 'in', 'the', 'will', 'into', 'above', 'doing', 'his', 'to', 't', 've', \"hadn't\", 'not', 'too', 'until', 'by', 'we', 'down', 'shan', 'or', \"you'll\", 'most', 'at', 'd', 'further', 'isn', 'yourselves', 'haven', 'but', 'below', 'its', 'those', 'and', 'shouldn', 'her', 'aren', 'couldn', 'these', 'hers', \"shan't\", \"shouldn't\", \"wasn't\", 'both', 'mustn', 'which', 'during', 'own', \"that'll\", 'ourselves', 'wouldn', 'y', 'just', \"it's\", 'some', 'such', 'if', \"didn't\", 'out', 'through', 'll', 'won', 'yourself', 'herself', 'myself', 'after', 'here', \"won't\", 'what', 'had', 'all', 'am', 'you', \"you're\", 'a', 'wasn', 'why', 'ain', 'only', 'then', 'is', 'did', 'be', \"haven't\"}\n"
     ]
    }
   ],
   "source": [
    "# stop words are basically all the extraneous words in each sentence that we can feasibly ignore\n",
    "# this would (ideally) leave us with just the main nouns, verbs and such in the sentence\n",
    "# we will use this in later functions\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f486ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean\n",
      "clean\n"
     ]
    }
   ],
   "source": [
    "# The PorterStemmer() function simply diminishes a word to its root base. \n",
    "# For example, \"cleaning\" and \"cleaned\" would both return as \"clean\"\n",
    "stem = PorterStemmer()\n",
    "wd = stem.stem(\"cleaning\")\n",
    "print(wd)\n",
    "wd1 = stem.stem(\"cleaned\")\n",
    "print(wd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e131349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create word frequency table\n",
    "def create_dictionary_table(sentence_text):\n",
    "    frequency_table = {}\n",
    "    word_list = word_tokenize(sentence_text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stem = PorterStemmer()\n",
    "    for word in word_list:\n",
    "        word = stem.stem(word)\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        if word in frequency_table: \n",
    "            frequency_table[word] += 1\n",
    "        else: \n",
    "            frequency_table[word] = 1\n",
    "    return frequency_table\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51891cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate sentence scores with weighted average occurrence of key words\n",
    "def calculate_sentence_scores(sentences, frequency_table):\n",
    "    sentence_weight = {}\n",
    "    for sentence in allSentences:\n",
    "        wordcount = len(word_tokenize(sentence))\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
