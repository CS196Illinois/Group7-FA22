{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a015e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the resources doc if you haven't already!\n",
    "# Also please add whatever interesting/useful sources you find on there for everyone else!\n",
    "# Also, PLEASE check the documentation file in the Project folder if you are confused about any of the NLTK functions\n",
    "\n",
    "# Text summarization tends to have two approaches: extraction and abstraction\n",
    "# Because abstraction is more complex, we can try to build an extraction algorithm first\n",
    "# Although I would definitely recommend checking it out to get an idea of how we can go forward from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c4c7441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For collaborators, right now I'm using the algorithm here:\n",
    "# https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48df224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TLDR; Our extraction algorithm will go like this:\n",
    "# -obtain data\n",
    "# -process text\n",
    "# -tokenization\n",
    "# -find weighted frequency of words (weigh by sentence length, paragraph length, etc)\n",
    "# -substitute words with their weighted frequencies (exactly what it sounds like)\n",
    "# -sum up the weighted frequencies in each sentence, and the sentences with highest sums make up our summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b967d231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jrh25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jrh25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Starting off with imports\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2410f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should try to make this less memory intensive\n",
    "# paragraphContent should be fetched from database file, but I'm filling it in with example text for testing purposes\n",
    "paragraphContent = \"\"\"Sally sells seashells by the seashore. She worries that she doesn't have enough seashells to sell on the seashore. She won't give up. Sally wants to sell seashells on the seashore, because she likes to sell seashells on the seashore. She is selling seashells on the seashell shore. How much wood could the woodchuck chuck if the woodchuck could chuck wood? She sells seashells by the seashore.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e131349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create word frequency table for the entire text of the paragraph\n",
    "def create_frequency_table(content):\n",
    "    frequency_table = {}\n",
    "    word_list = word_tokenize(content)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stem = PorterStemmer()\n",
    "    for word in word_list:\n",
    "        word = stem.stem(word)\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        if word in frequency_table: \n",
    "            frequency_table[word] += 1\n",
    "        else: \n",
    "            frequency_table[word] = 1\n",
    "# making punctuation have 0 frequency to prevent them from skewing our weighted frequency\n",
    "    punctuation = {\";\", \":\", \"'\", \".\", \",\", \"!\", \"?\", \"(\", \")\"}\n",
    "    for word in frequency_table:\n",
    "        if word in punctuation:\n",
    "            frequency_table[word] = 0\n",
    "    return frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07aff4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salli\t2\n",
      "sell\t6\n",
      "seashel\t7\n",
      "seashor\t5\n",
      ".\t0\n",
      "worri\t1\n",
      "doe\t1\n",
      "n't\t2\n",
      "enough\t1\n",
      "wo\t1\n",
      "give\t1\n",
      "want\t1\n",
      ",\t0\n",
      "becaus\t1\n",
      "like\t1\n",
      "shore\t1\n",
      "much\t1\n",
      "wood\t2\n",
      "could\t2\n",
      "woodchuck\t2\n",
      "chuck\t2\n",
      "?\t0\n"
     ]
    }
   ],
   "source": [
    "freq_dict = create_frequency_table(paragraphContent)\n",
    "for i in freq_dict:\n",
    "    print(i + \"\\t\" + str(freq_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83feb869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in the word frequency dictionary from create_dictionary_table\n",
    "def create_weighted_table(frequency_table):\n",
    "    weighted_frequency_table = {}\n",
    "    highestfreq = max(frequency_table.values())\n",
    "    for word in frequency_table:\n",
    "        weighted_frequency_table[word] = frequency_table[word] / highestfreq\n",
    "    return weighted_frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72412516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salli\t0.2857142857142857\n",
      "sell\t0.8571428571428571\n",
      "seashel\t1.0\n",
      "seashor\t0.7142857142857143\n",
      ".\t0.0\n",
      "worri\t0.14285714285714285\n",
      "doe\t0.14285714285714285\n",
      "n't\t0.2857142857142857\n",
      "enough\t0.14285714285714285\n",
      "wo\t0.14285714285714285\n",
      "give\t0.14285714285714285\n",
      "want\t0.14285714285714285\n",
      ",\t0.0\n",
      "becaus\t0.14285714285714285\n",
      "like\t0.14285714285714285\n",
      "shore\t0.14285714285714285\n",
      "much\t0.14285714285714285\n",
      "wood\t0.2857142857142857\n",
      "could\t0.2857142857142857\n",
      "woodchuck\t0.2857142857142857\n",
      "chuck\t0.2857142857142857\n",
      "?\t0.0\n"
     ]
    }
   ],
   "source": [
    "weighted_dict = create_weighted_table(freq_dict)\n",
    "for i in weighted_dict:\n",
    "    print(i + \"\\t\" + str(weighted_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bee0cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutParagraph(content):\n",
    "    allSentences = sent_tokenize(content)\n",
    "    return allSentences;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e69fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitute words in each sentence with weighted frequencies, \n",
    "# sum up the weighted word frequencies in each sentence, these sums will be the sentence's \"score\"\n",
    "# compare the sentence scores, and grab the ones with the highest scores for our summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "924e2ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['salli', 'sell', 'seashel', 'seashor', '.'], ['worri', 'doe', \"n't\", 'enough', 'seashel', 'sell', 'seashor', '.'], ['wo', \"n't\", 'give', '.'], ['salli', 'want', 'sell', 'seashel', 'seashor', ',', 'becaus', 'like', 'sell', 'seashel', 'seashor', '.'], ['sell', 'seashel', 'seashel', 'shore', '.'], ['much', 'wood', 'could', 'woodchuck', 'chuck', 'woodchuck', 'could', 'chuck', 'wood', '?'], ['sell', 'seashel', 'seashor', '.']]\n"
     ]
    }
   ],
   "source": [
    "# clean sentences using PorterStemmer() and getting rid of stop words, similar to tokenization\n",
    "# we can then calculate the scores in each sentence based on the \"weighted frequencies\" that we\n",
    "# assigned to each of these words in our previously made dictionary\n",
    "def clean_sentences(content):\n",
    "    allSentences = sent_tokenize(content)\n",
    "    stem = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    all_clean_sentences = []\n",
    "    for sentence in allSentences:\n",
    "        cleaned_sentence = []\n",
    "        word_list = word_tokenize(sentence)\n",
    "        for word in word_list:\n",
    "            word = stem.stem(word)\n",
    "            if (word not in stop_words):\n",
    "                cleaned_sentence.append(word)\n",
    "        all_clean_sentences.append(cleaned_sentence)\n",
    "    return all_clean_sentences\n",
    "print(clean_sentences(paragraphContent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e2e83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the frequencies of each sentence, and returns the sentence with the greatest frequency \n",
    "\n",
    "def sentence_scores(content):\n",
    "    freq_dict = create_frequency_table(content)\n",
    "    weighted_dict = create_weighted_table(freq_dict)\n",
    "    cleanedSentences = clean_sentences(content)\n",
    "    addedNum = []\n",
    "    \n",
    "    for clean_sentence in cleanedSentences:\n",
    "        num = 0\n",
    "        for token in clean_sentence:\n",
    "            if token in weighted_dict.keys():\n",
    "                num = num + weighted_dict[token]\n",
    "        addedNum.append(num)\n",
    "    \n",
    "    print(addedNum)\n",
    "    \n",
    "    largest = addedNum[0]\n",
    "    numTracker = 0\n",
    "    finalNum = 0\n",
    "    for i in addedNum:\n",
    "        numTracker = numTracker + 1\n",
    "        if i > largest:\n",
    "            largest = i\n",
    "            finalNum = numTracker\n",
    "    allSentences = cutParagraph(paragraphContent)\n",
    "    return allSentences[finalNum - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d741e4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.857142857142857, 3.2857142857142856, 0.5714285714285714, 5.857142857142857, 3.0, 2.428571428571428, 2.5714285714285716]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sally wants to sell seashells on the seashore, because she likes to sell seashells on the seashore.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_scores(paragraphContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81a129c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the frequencies of each sentence, and returns the sentence with the greatest frequency \n",
    "\n",
    "def sentence_scores(content):\n",
    "    freq_dict = create_frequency_table(content)\n",
    "    weighted_dict = create_weighted_table(freq_dict)\n",
    "    allSentences = cutParagraph(content)\n",
    "    addedNum = []\n",
    "    \n",
    "    for i in allSentences:\n",
    "        num = 0\n",
    "        words = i.split(\" \")\n",
    "        for j in words:\n",
    "            if j in weighted_dict.keys():\n",
    "                num = num + weighted_dict[j]\n",
    "        addedNum.append(num)\n",
    "    \n",
    "    print(addedNum)\n",
    "    largest = addedNum[0]\n",
    "    numTracker = 0\n",
    "    finalNum = 0\n",
    "    for i in addedNum:\n",
    "        numTracker = numTracker + 1\n",
    "        if i > largest:\n",
    "            largest = i\n",
    "            finalNum = numTracker\n",
    "    allSentences = cutParagraph(paragraphContent)\n",
    "    return allSentences[finalNum - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc4ba8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1.0, 0.14285714285714285, 1.7142857142857142, 0, 2.1428571428571423, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How much wood could the woodchuck chuck if the woodchuck could chuck wood?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_scores(paragraphContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39f5894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
