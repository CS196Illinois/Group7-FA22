{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "1a015e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the resources doc if you haven't already!\n",
    "# Also please add whatever interesting/useful sources you find on there for everyone else!\n",
    "# Also, PLEASE check the documentation file in the Project folder if you are confused about any of the NLTK functions\n",
    "\n",
    "# Text summarization tends to have two approaches: extraction and abstraction\n",
    "# Because abstraction is more complex, we can try to build an extraction algorithm first\n",
    "# Although I would definitely recommend checking it out to get an idea of how we can go forward from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "6c4c7441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For collaborators, right now I'm using the algorithm here:\n",
    "# https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "c48df224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TLDR; Our extraction algorithm will go like this:\n",
    "# -obtain data\n",
    "# -process text\n",
    "# -tokenization\n",
    "# -find weighted frequency of words (weigh by sentence length, paragraph length, etc)\n",
    "# -substitute words with their weighted frequencies (exactly what it sounds like)\n",
    "# -sum up the weighted frequencies in each sentence, and the sentences with highest sums make up our summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "b967d231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/karengao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/karengao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Starting off with imports\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "a2410f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should try to make this less memory intensive\n",
    "# paragraphContent should be fetched from database file, but I'm filling it in with example text for testing purposes\n",
    "paragraphContent = \"\"\"Sally sells seashells by the seashore. She worries that she doesn't have enough seashells to sell on the seashore. She won't give up. Sally wants to sell seashells on the seashore, because she likes to sell seashells on the seashore. She is selling seashells on the seashell shore. How much wood could the woodchuck chuck if the woodchuck could chuck wood? She sells seashells by the seashore.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "3e131349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create word frequency table for the entire text of the paragraph\n",
    "def create_frequency_table(content):\n",
    "    frequency_table = {}\n",
    "    word_list = word_tokenize(content)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stem = PorterStemmer()\n",
    "    for word in word_list:\n",
    "        word = stem.stem(word)\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        if word in frequency_table: \n",
    "            frequency_table[word] += 1\n",
    "        else: \n",
    "            frequency_table[word] = 1\n",
    "# making punctuation have 0 frequency to prevent them from skewing our weighted frequency\n",
    "    punctuation = {\";\", \":\", \"'\", \".\", \",\", \"!\", \"?\", \"(\", \")\"}\n",
    "    for word in frequency_table:\n",
    "        if word in punctuation:\n",
    "            frequency_table[word] = 0\n",
    "    return frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "07aff4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salli\t2\n",
      "sell\t6\n",
      "seashel\t7\n",
      "seashor\t5\n",
      ".\t0\n",
      "worri\t1\n",
      "doe\t1\n",
      "n't\t2\n",
      "enough\t1\n",
      "wo\t1\n",
      "give\t1\n",
      "want\t1\n",
      ",\t0\n",
      "becaus\t1\n",
      "like\t1\n",
      "shore\t1\n",
      "much\t1\n",
      "wood\t2\n",
      "could\t2\n",
      "woodchuck\t2\n",
      "chuck\t2\n",
      "?\t0\n"
     ]
    }
   ],
   "source": [
    "freq_dict = create_frequency_table(paragraphContent)\n",
    "for i in freq_dict:\n",
    "    print(i + \"\\t\" + str(freq_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "83feb869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in the word frequency dictionary from create_dictionary_table\n",
    "def create_weighted_table(frequency_table):\n",
    "    weighted_frequency_table = {}\n",
    "    highestfreq = max(frequency_table.values())\n",
    "    for word in frequency_table:\n",
    "        weighted_frequency_table[word] = frequency_table[word] / highestfreq\n",
    "    return weighted_frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "72412516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salli\t0.2857142857142857\n",
      "sell\t0.8571428571428571\n",
      "seashel\t1.0\n",
      "seashor\t0.7142857142857143\n",
      ".\t0.0\n",
      "worri\t0.14285714285714285\n",
      "doe\t0.14285714285714285\n",
      "n't\t0.2857142857142857\n",
      "enough\t0.14285714285714285\n",
      "wo\t0.14285714285714285\n",
      "give\t0.14285714285714285\n",
      "want\t0.14285714285714285\n",
      ",\t0.0\n",
      "becaus\t0.14285714285714285\n",
      "like\t0.14285714285714285\n",
      "shore\t0.14285714285714285\n",
      "much\t0.14285714285714285\n",
      "wood\t0.2857142857142857\n",
      "could\t0.2857142857142857\n",
      "woodchuck\t0.2857142857142857\n",
      "chuck\t0.2857142857142857\n",
      "?\t0.0\n"
     ]
    }
   ],
   "source": [
    "weighted_dict = create_weighted_table(freq_dict)\n",
    "for i in weighted_dict:\n",
    "    print(i + \"\\t\" + str(weighted_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "cb4e13f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sally sells seashells by the seashore.\n",
      "She worries that she doesn't have enough seashells to sell on the seashore.\n",
      "She won't give up.\n",
      "Sally wants to sell seashells on the seashore, because she likes to sell seashells on the seashore.\n",
      "She is selling seashells on the seashell shore.\n",
      "How much wood could the woodchuck chuck if the woodchuck could chuck wood?\n",
      "She sells seashells by the seashore.\n",
      "['Sally', 'sells', 'seashells', 'by', 'the', 'seashore', '.']\n",
      "['She', 'worries', 'that', 'she', 'does', \"n't\", 'have', 'enough', 'seashells', 'to', 'sell', 'on', 'the', 'seashore', '.']\n",
      "['She', 'wo', \"n't\", 'give', 'up', '.']\n",
      "['Sally', 'wants', 'to', 'sell', 'seashells', 'on', 'the', 'seashore', ',', 'because', 'she', 'likes', 'to', 'sell', 'seashells', 'on', 'the', 'seashore', '.']\n",
      "['She', 'is', 'selling', 'seashells', 'on', 'the', 'seashell', 'shore', '.']\n",
      "['How', 'much', 'wood', 'could', 'the', 'woodchuck', 'chuck', 'if', 'the', 'woodchuck', 'could', 'chuck', 'wood', '?']\n",
      "['She', 'sells', 'seashells', 'by', 'the', 'seashore', '.']\n"
     ]
    }
   ],
   "source": [
    "def cutParagraph(content):\n",
    "    allSentences = sent_tokenize(content)\n",
    "    return allSentences;\n",
    "allSentences = cutParagraph(paragraphContent)\n",
    "for sentence in allSentences:\n",
    "    print(sentence)\n",
    "for sentence in allSentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "0e69fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitute words in each sentence with weighted frequencies, \n",
    "# pretty damn sure PortStemmer() fucked up the words somehow\n",
    "# sum up the weighted word frequencies in each sentence, these sums will be the sentence's \"score\"\n",
    "# compare the sentence scores, and grab the ones with the highest scores for our summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "81a129c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the frequencies of each sentence, and returns the sentence with the greatest frequency \n",
    "\n",
    "def sentence_scores(content):\n",
    "    weighted_dict = create_weighted_table(freq_dict)\n",
    "    addedNum = []\n",
    "    num = 0\n",
    "    \n",
    "    for i in weighted_dict:\n",
    "        if weighted_dict.get(i) == 0.0:\n",
    "            addedNum.append(num)\n",
    "            num = 0\n",
    "        else:\n",
    "            num = num + weighted_dict[i]\n",
    "    \n",
    "    largest = addedNum[0]\n",
    "    numTracker = 0\n",
    "    finalNum = 0\n",
    "    for i in addedNum:\n",
    "        numTracker = numTracker + 1\n",
    "        if i > largest:\n",
    "            largest = i\n",
    "            finalNum = numTracker\n",
    "    allSentences = cutParagraph(paragraphContent)\n",
    "    return allSentences[finalNum - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "dc4ba8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'She sells seashells by the seashore.'"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_scores(paragraphContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee0cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
